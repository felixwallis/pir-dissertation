{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "CONGRESSIONAL_RECORD_PATH = '../../congressional-record/dist/'\n",
    "HANSARD_PATH = '../../hansard-in-full/'\n",
    "CLIMATE_DICTIONARY_PATH = '../dictionaries/dist/'\n",
    "CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH = '../dictionaries/dist/'\n",
    "HANSARD_PROCEDURAL_STEMS_PATH = '../dictionaries/dist/'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "\n",
    "YEAR_RANGE = (1997, 2015)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    stemmed_tokens_with_original = [\n",
    "        (token, stemmer.stem(token)) for token in filtered_tokens]\n",
    "    return stemmed_tokens_with_original\n",
    "\n",
    "\n",
    "def corpus_cleaning_tokenizing_stemming(corpus: pd.DataFrame, text_column_name: str, year: int):\n",
    "    corpus = corpus.copy()\n",
    "\n",
    "    corpus = corpus[corpus['year'] == year]\n",
    "\n",
    "    tqdm.pandas(desc=f\"Processing Text for Year {year}\")\n",
    "    corpus['cleaned_stems_with_original'] = corpus[text_column_name].progress_apply(\n",
    "        tokenize_and_stem)\n",
    "\n",
    "    # Remove any documents with fewer than 30 stems\n",
    "    corpus['stem_count'] = corpus['cleaned_stems_with_original'].apply(len)\n",
    "    corpus = corpus[corpus['stem_count'] >= 30]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def process_dataframe(df, text_column_name, year_range, data_path):\n",
    "    temp_dir = os.path.join(data_path, \"temp_yearly_dataframes\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    for year in range(year_range[0], year_range[1] + 1):\n",
    "        yearly_df = corpus_cleaning_tokenizing_stemming(\n",
    "            df, text_column_name, year)\n",
    "        yearly_df.to_pickle(os.path.join(\n",
    "            temp_dir, f\"congressional_record_{year}.pkl\"))\n",
    "\n",
    "    return temp_dir\n",
    "\n",
    "\n",
    "def concatenate_dataframes(temp_dir, year_range):\n",
    "    yearly_dataframes = []\n",
    "\n",
    "    for year in range(year_range[0], year_range[1] + 1):\n",
    "        print(f\"Loading Year {year}\")\n",
    "        yearly_df = pd.read_pickle(os.path.join(\n",
    "            temp_dir, f\"congressional_record_{year}.pkl\"))\n",
    "        yearly_dataframes.append(yearly_df)\n",
    "        del yearly_df  # Free memory by deleting the yearly dataframe after appending\n",
    "\n",
    "    processed_df = pd.concat(yearly_dataframes, ignore_index=True)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def corpus_filtering(corpus: pd.DataFrame, min_df: int = 10):\n",
    "    # Create a set of allowed stems based on the min_df threshold\n",
    "    vectorizer = CountVectorizer(min_df=min_df)\n",
    "    vectorizer.fit_transform(corpus['cleaned_stems_with_original'].apply(\n",
    "        lambda x: ' '.join([stem for _, stem in x])))\n",
    "    allowed_stems = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Filter out stems that occur in less than min_df documents\n",
    "    tqdm.pandas(desc=\"Filtering Stems\")\n",
    "    corpus['cleaned_stems_with_original'] = corpus['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda x: [(token, stem) for token, stem in x if stem in allowed_stems])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PATH + 'congressional_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text for Year 1997: 100%|██████████| 69432/69432 [01:29<00:00, 775.46it/s] \n",
      "Processing Text for Year 1998: 100%|██████████| 71324/71324 [01:29<00:00, 796.31it/s] \n",
      "Processing Text for Year 1999: 100%|██████████| 75469/75469 [01:38<00:00, 768.71it/s] \n",
      "Processing Text for Year 2000: 100%|██████████| 65399/65399 [01:27<00:00, 748.94it/s] \n",
      "Processing Text for Year 2001: 100%|██████████| 63817/63817 [01:27<00:00, 732.94it/s] \n",
      "Processing Text for Year 2002: 100%|██████████| 52146/52146 [01:12<00:00, 715.58it/s] \n",
      "Processing Text for Year 2003: 100%|██████████| 73362/73362 [01:39<00:00, 733.75it/s] \n",
      "Processing Text for Year 2004: 100%|██████████| 53941/53941 [01:16<00:00, 705.87it/s] \n",
      "Processing Text for Year 2005: 100%|██████████| 64182/64182 [01:31<00:00, 702.87it/s] \n",
      "Processing Text for Year 2006: 100%|██████████| 52510/52510 [01:12<00:00, 722.29it/s] \n",
      "Processing Text for Year 2007: 100%|██████████| 80177/80177 [01:48<00:00, 737.61it/s] \n",
      "Processing Text for Year 2008: 100%|██████████| 48791/48791 [01:10<00:00, 689.24it/s] \n",
      "Processing Text for Year 2009: 100%|██████████| 47103/47103 [01:04<00:00, 726.06it/s] \n",
      "Processing Text for Year 2010: 100%|██████████| 46918/46918 [01:08<00:00, 680.51it/s] \n",
      "Processing Text for Year 2011: 100%|██████████| 51123/51123 [01:11<00:00, 713.02it/s] \n",
      "Processing Text for Year 2012: 100%|██████████| 38664/38664 [00:54<00:00, 709.17it/s] \n",
      "Processing Text for Year 2013: 100%|██████████| 42288/42288 [01:01<00:00, 685.33it/s] \n",
      "Processing Text for Year 2014: 100%|██████████| 35439/35439 [00:50<00:00, 703.65it/s] \n",
      "Processing Text for Year 2015: 100%|██████████| 42867/42867 [00:58<00:00, 727.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Year 1997\n",
      "Loading Year 1998\n",
      "Loading Year 1999\n",
      "Loading Year 2000\n",
      "Loading Year 2001\n",
      "Loading Year 2002\n",
      "Loading Year 2003\n",
      "Loading Year 2004\n",
      "Loading Year 2005\n",
      "Loading Year 2006\n",
      "Loading Year 2007\n",
      "Loading Year 2008\n",
      "Loading Year 2009\n",
      "Loading Year 2010\n",
      "Loading Year 2011\n",
      "Loading Year 2012\n",
      "Loading Year 2013\n",
      "Loading Year 2014\n",
      "Loading Year 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Stems: 100%|██████████| 526039/526039 [04:51<00:00, 1805.84it/s] \n"
     ]
    }
   ],
   "source": [
    "congressional_record['date'] = pd.to_datetime(\n",
    "    congressional_record['date'], format='%Y%m%d')\n",
    "congressional_record['year'] = congressional_record['date'].dt.year\n",
    "\n",
    "# Remove any speeches with missing speaker IDs\n",
    "congressional_record = congressional_record.dropna(subset=['speaker_id'])\n",
    "\n",
    "# Remove any speeches that are not from Democrats or Republicans\n",
    "congressional_record = congressional_record[\n",
    "    congressional_record['party'].isin(['D', 'R'])]\n",
    "\n",
    "temp_dir = process_dataframe(\n",
    "    congressional_record, 'speech', YEAR_RANGE, DATA_PATH)\n",
    "\n",
    "processed_congressional_record = concatenate_dataframes(\n",
    "    'data/temp_yearly_dataframes', YEAR_RANGE)\n",
    "\n",
    "processed_congressional_record = corpus_filtering(\n",
    "    processed_congressional_record)\n",
    "\n",
    "processed_congressional_record.to_parquet(\n",
    "    DATA_PATH + 'congressional_record.parquet')\n",
    "\n",
    "# Free memory by deleting the processed dataframe\n",
    "del processed_congressional_record\n",
    "\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(HANSARD_PATH + 'hansard_with_mp_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text for Year 1997: 100%|██████████| 24339/24339 [00:26<00:00, 922.33it/s] \n",
      "Processing Text for Year 1998: 100%|██████████| 51348/51348 [00:51<00:00, 995.76it/s] \n",
      "Processing Text for Year 1999: 100%|██████████| 47274/47274 [00:48<00:00, 968.70it/s] \n",
      "Processing Text for Year 2000: 100%|██████████| 48736/48736 [00:46<00:00, 1042.47it/s]\n",
      "Processing Text for Year 2001: 100%|██████████| 39627/39627 [00:26<00:00, 1493.44it/s]\n",
      "Processing Text for Year 2002: 100%|██████████| 40057/40057 [00:14<00:00, 2857.01it/s]\n",
      "Processing Text for Year 2003: 100%|██████████| 44968/44968 [00:16<00:00, 2777.20it/s]\n",
      "Processing Text for Year 2004: 100%|██████████| 45883/45883 [00:16<00:00, 2751.11it/s]\n",
      "Processing Text for Year 2005: 100%|██████████| 43149/43149 [00:15<00:00, 2715.22it/s]\n",
      "Processing Text for Year 2006: 100%|██████████| 49239/49239 [00:18<00:00, 2667.33it/s]\n",
      "Processing Text for Year 2007: 100%|██████████| 51223/51223 [00:19<00:00, 2578.38it/s]\n",
      "Processing Text for Year 2008: 100%|██████████| 53520/53520 [00:19<00:00, 2717.96it/s]\n",
      "Processing Text for Year 2009: 100%|██████████| 51038/51038 [00:19<00:00, 2654.18it/s]\n",
      "Processing Text for Year 2010: 100%|██████████| 51744/51744 [00:17<00:00, 2909.41it/s]\n",
      "Processing Text for Year 2011: 100%|██████████| 66299/66299 [00:21<00:00, 3034.62it/s]\n",
      "Processing Text for Year 2012: 100%|██████████| 53061/53061 [00:16<00:00, 3136.98it/s]\n",
      "Processing Text for Year 2013: 100%|██████████| 57145/57145 [00:18<00:00, 3142.43it/s]\n",
      "Processing Text for Year 2014: 100%|██████████| 52162/52162 [00:17<00:00, 3017.94it/s]\n",
      "Processing Text for Year 2015: 100%|██████████| 49568/49568 [00:16<00:00, 3068.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Year 1997\n",
      "Loading Year 1998\n",
      "Loading Year 1999\n",
      "Loading Year 2000\n",
      "Loading Year 2001\n",
      "Loading Year 2002\n",
      "Loading Year 2003\n",
      "Loading Year 2004\n",
      "Loading Year 2005\n",
      "Loading Year 2006\n",
      "Loading Year 2007\n",
      "Loading Year 2008\n",
      "Loading Year 2009\n",
      "Loading Year 2010\n",
      "Loading Year 2011\n",
      "Loading Year 2012\n",
      "Loading Year 2013\n",
      "Loading Year 2014\n",
      "Loading Year 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Stems: 100%|██████████| 473093/473093 [00:29<00:00, 16056.36it/s] \n"
     ]
    }
   ],
   "source": [
    "hansard['speech_date'] = pd.to_datetime(hansard['speech_date'])\n",
    "hansard['year'] = hansard['speech_date'].dt.year\n",
    "\n",
    "# Remove any speeches with missing memberships\n",
    "hansard = hansard.dropna(subset=['memberships'])\n",
    "\n",
    "# Clean party names\n",
    "hansard['speech_party'] = hansard['speech_party'].replace(\n",
    "    {'Labour/Co-operative': 'Labour', 'Independent Labour': 'Labour', 'Independent Conservative': 'Conservative'})\n",
    "\n",
    "# Remove any speeches that are not from Labour or Conservative MPs\n",
    "hansard = hansard[hansard['speech_party'].isin(['Labour', 'Conservative'])]\n",
    "\n",
    "temp_dir = process_dataframe(hansard, 'text', YEAR_RANGE, DATA_PATH)\n",
    "\n",
    "processed_hansard = concatenate_dataframes(temp_dir, YEAR_RANGE)\n",
    "\n",
    "processed_hansard = corpus_filtering(processed_hansard)\n",
    "\n",
    "processed_hansard.to_parquet(DATA_PATH + 'hansard.parquet')\n",
    "\n",
    "# Free memory by deleting the processed dataframe\n",
    "del processed_hansard\n",
    "\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_proportion(doc_stems, terms):\n",
    "    doc_tokens = set(doc_stems)\n",
    "    term_count = sum(term in doc_tokens for term in terms)\n",
    "    total_terms = len(doc_tokens)\n",
    "    if total_terms == 0:\n",
    "        return 0\n",
    "    return term_count / total_terms\n",
    "\n",
    "\n",
    "def procedural_stems_filter(corpus_df, procedural_stems, threshold: float = 0.5):\n",
    "    corpus_df = corpus_df.copy()\n",
    "\n",
    "    # Remove documents that contain more than the threshold of procedural stems\n",
    "    tqdm.pandas(desc=\"Calculating procedural proportion\")\n",
    "    corpus_df['procedural_proportion'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: term_proportion([stem for _, stem in doc], procedural_stems))\n",
    "    corpus_df = corpus_df[corpus_df['procedural_proportion'] < threshold]\n",
    "\n",
    "    # Remove all procedural stems from the remaining documents\n",
    "    tqdm.pandas(desc=\"Removing procedural stems\")\n",
    "    corpus_df['cleaned_stems_with_original'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: [(token, stem) for token, stem in doc if stem not in procedural_stems])\n",
    "\n",
    "    tqdm.pandas(desc=\"Joining cleaned stems\")\n",
    "    corpus_df['cleaned_stems'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: ' '.join([stem for _, stem in doc]))\n",
    "\n",
    "    tqdm.pandas(desc=\"Joining cleaned tokens\")\n",
    "    corpus_df['cleaned_tokens'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: ' '.join([token for token, _ in doc]))\n",
    "\n",
    "    corpus_df = corpus_df.drop(columns=['cleaned_stems_with_original'])\n",
    "\n",
    "    return corpus_df\n",
    "\n",
    "\n",
    "def topic_stems_filter(corpus_df, topic_stems, threshold: float = 0.2):\n",
    "    corpus_df = corpus_df.copy()\n",
    "\n",
    "    # Remove any documents that contain fewer than the threshold proportion of topic stems\n",
    "    corpus_df['topic_proportion'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: term_proportion(doc.split(), topic_stems)\n",
    "    )\n",
    "    corpus_df = corpus_df[corpus_df['topic_proportion'] > threshold]\n",
    "\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing procedural documents and stems from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_parquet(\n",
    "    DATA_PATH + 'congressional_record.parquet')\n",
    "hansard = pd.read_parquet(DATA_PATH + 'hansard.parquet')\n",
    "\n",
    "congressional_record_procedural_stems = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH + 'shortened_congressional_record_procedural_stems.csv')\n",
    "congressional_record_procedural_stems = set(\n",
    "    congressional_record_procedural_stems['stem'].tolist())\n",
    "\n",
    "hansard_procedural_stems = pd.read_csv(\n",
    "    HANSARD_PROCEDURAL_STEMS_PATH + 'expanded_hansard_procedural_stems.csv')\n",
    "hansard_procedural_stems = set(hansard_procedural_stems['stem'].tolist())\n",
    "\n",
    "climate_stems = pd.read_csv(\n",
    "    CLIMATE_DICTIONARY_PATH + 'shortened_climate_stems.csv')\n",
    "climate_stems = set(climate_stems['stem'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating procedural proportion: 100%|██████████| 526039/526039 [01:51<00:00, 4721.04it/s]\n",
      "Removing procedural stems: 100%|██████████| 478172/478172 [01:35<00:00, 5000.94it/s]\n",
      "Joining cleaned stems: 100%|██████████| 478172/478172 [00:07<00:00, 63469.07it/s]\n",
      "Joining cleaned tokens: 100%|██████████| 478172/478172 [00:07<00:00, 67476.77it/s]\n"
     ]
    }
   ],
   "source": [
    "non_procedural_congressional_record = procedural_stems_filter(\n",
    "    congressional_record, congressional_record_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating procedural proportion: 100%|██████████| 473093/473093 [00:26<00:00, 17985.83it/s]\n",
      "Removing procedural stems: 100%|██████████| 448828/448828 [00:14<00:00, 31575.21it/s]\n",
      "Joining cleaned stems: 100%|██████████| 448828/448828 [00:01<00:00, 439997.74it/s]\n",
      "Joining cleaned tokens: 100%|██████████| 448828/448828 [00:01<00:00, 432115.09it/s]\n"
     ]
    }
   ],
   "source": [
    "non_procedural_hansard = procedural_stems_filter(\n",
    "    hansard, hansard_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_hansard.to_csv(\n",
    "    DATA_PATH + 'non_procedural_hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating climate change documents from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = pd.read_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv')\n",
    "\n",
    "climate_congressional_record = topic_stems_filter(\n",
    "    non_procedural_congressional_record, climate_stems, 0.02)\n",
    "climate_congressional_record = climate_congressional_record.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'climate_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = pd.read_csv(DATA_PATH + 'non_procedural_hansard.csv')\n",
    "\n",
    "climate_hansard = topic_stems_filter(\n",
    "    non_procedural_hansard, climate_stems, 0.025)\n",
    "climate_hansard = climate_hansard.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_hansard.to_csv(DATA_PATH + 'climate_hansard.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
