{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code needed to filter the Congressional Record and Hansard datasets. The code creates two pairs of corpuses with cleaned and stemmed text. The first pair contains only speeches from the Congressional Record and Hansard that discuss issues relating to climate change. The second contains all speeches made between 1997 and 2015 for each corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "CONGRESSIONAL_RECORD_PATH = '../../congressional-record/dist/'\n",
    "HANSARD_PATH = '../../hansard-in-full/'\n",
    "CLIMATE_DICTIONARY_PATH = '../dictionaries/dist/'\n",
    "CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH = '../dictionaries/dist/'\n",
    "HANSARD_PROCEDURAL_STEMS_PATH = '../dictionaries/dist/'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "\n",
    "YEAR_RANGE = (1997, 2015)\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # Text should almost always be a string, but we check just in case\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stem the tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Rejoin the stemmed tokens\n",
    "    joined_stems = ' '.join(stemmed_tokens)\n",
    "    return joined_stems\n",
    "\n",
    "\n",
    "def corpus_preprocessing(corpus: pd.DataFrame,\n",
    "                         text_column_name: str,\n",
    "                         year_range: tuple,\n",
    "                         min_df: int = 10,\n",
    "                         year_column_name: str = 'year'):\n",
    "    corpus = corpus.copy()\n",
    "    # Remove corpus content from outside the year range\n",
    "    corpus = corpus[corpus[year_column_name].between(\n",
    "        year_range[0], year_range[1])]\n",
    "    # Clean, tokenize, and stem the corpus\n",
    "    tqdm.pandas(desc=\"Processing Text\")\n",
    "    corpus['cleaned_stems'] = corpus[text_column_name].progress_apply(\n",
    "        tokenize_and_stem)\n",
    "    # Remove any documents with fewer than 10 stems\n",
    "    corpus['stem_count'] = corpus['cleaned_stems'].apply(\n",
    "        lambda x: len(x.split()))\n",
    "    corpus = corpus[corpus['stem_count'] >= 50]\n",
    "\n",
    "    # Remove stems that occur in less than min_df documents\n",
    "    vectorizer = CountVectorizer(min_df=min_df)\n",
    "    vectorizer.fit_transform(corpus['cleaned_stems'])\n",
    "    allowed_words = set(vectorizer.get_feature_names_out())\n",
    "    corpus['cleaned_stems'] = corpus['cleaned_stems'].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word in allowed_words]))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PATH + 'congressional_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 1080262/1080262 [25:44<00:00, 699.29it/s] \n"
     ]
    }
   ],
   "source": [
    "congressional_record['date'] = pd.to_datetime(\n",
    "    congressional_record['date'], format='%Y%m%d')\n",
    "congressional_record['year'] = congressional_record['date'].dt.year\n",
    "\n",
    "# Remove any speeches with missing speaker_id\n",
    "congressional_record = congressional_record.dropna(subset=['speaker_id'])\n",
    "\n",
    "congressional_record = corpus_preprocessing(\n",
    "    congressional_record, 'speech', YEAR_RANGE)\n",
    "\n",
    "congressional_record.to_csv(\n",
    "    DATA_PATH + 'congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(HANSARD_PATH + 'hansard_with_mp_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 1110136/1110136 [09:30<00:00, 1945.29it/s]\n"
     ]
    }
   ],
   "source": [
    "hansard['speech_date'] = pd.to_datetime(hansard['speech_date'])\n",
    "hansard['year'] = hansard['speech_date'].dt.year\n",
    "\n",
    "# Remove any speeches with missing memberships\n",
    "hansard = hansard.dropna(subset=['memberships'])\n",
    "\n",
    "hansard = corpus_preprocessing(hansard, 'text', YEAR_RANGE)\n",
    "\n",
    "hansard.to_csv(DATA_PATH + 'hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(DATA_PATH + 'hansard.csv')\n",
    "congressional_record = pd.read_csv(DATA_PATH + 'congressional_record.csv')\n",
    "\n",
    "congressional_record_procedural_stems = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH + 'shortened_congressional_record_procedural_stems.csv')\n",
    "congressional_record_procedural_stems = set(\n",
    "    congressional_record_procedural_stems['stem'].tolist())\n",
    "\n",
    "hansard_procedural_stems = pd.read_csv(\n",
    "    HANSARD_PROCEDURAL_STEMS_PATH + 'expanded_hansard_procedural_stems.csv')\n",
    "hansard_procedural_stems = set(hansard_procedural_stems['stem'].tolist())\n",
    "\n",
    "climate_stems = pd.read_csv(\n",
    "    CLIMATE_DICTIONARY_PATH + 'shortened_climate_stems.csv')\n",
    "climate_stems = set(climate_stems['stem'].tolist())\n",
    "\n",
    "\n",
    "def term_proportion(doc, terms):\n",
    "    doc_tokens = set(doc.split())\n",
    "    term_count = sum(\n",
    "        term in doc_tokens for term in terms)\n",
    "    total_terms = len(doc_tokens)\n",
    "\n",
    "    if total_terms == 0:\n",
    "        return 0\n",
    "    return term_count / total_terms\n",
    "\n",
    "\n",
    "def procedural_stems_filter(corpus_df, procedural_stems, threshold: float = 0.5):\n",
    "    corpus_df = corpus_df.copy()\n",
    "    # Remove documents that contain more than than the threshold of procedural stems\n",
    "    corpus_df['procedural_proportion'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: term_proportion(doc, procedural_stems))\n",
    "    corpus_df = corpus_df[corpus_df['procedural_proportion'] < threshold]\n",
    "    # Remove all procedural stems from the remaining documents\n",
    "    corpus_df['cleaned_stems'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: ' '.join([word for word in doc.split() if word not in procedural_stems]))\n",
    "    return corpus_df\n",
    "\n",
    "\n",
    "def topic_stems_filter(corpus_df, topic_stems, threshold: float = 0.2):\n",
    "    corpus_df = corpus_df.copy()\n",
    "    # Remove any documents that contain fewer than the threshold proportion of topic stems\n",
    "    corpus_df['topic_proportion'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: term_proportion(doc, topic_stems))\n",
    "    corpus_df = corpus_df[corpus_df['topic_proportion'] > threshold]\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing procedural documents and stems from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = procedural_stems_filter(\n",
    "    congressional_record, congressional_record_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = procedural_stems_filter(\n",
    "    hansard, hansard_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_hansard.to_csv(\n",
    "    DATA_PATH + 'non_procedural_hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating climate change documents from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = pd.read_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv')\n",
    "\n",
    "climate_congressional_record = topic_stems_filter(\n",
    "    non_procedural_congressional_record, climate_stems, 0.02)\n",
    "climate_congressional_record = climate_congressional_record.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_indices = climate_congressional_record.index\n",
    "non_climate_congressional_record = non_procedural_congressional_record.drop(\n",
    "    climate_indices)\n",
    "\n",
    "climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'climate_congressional_record.csv', index=False)\n",
    "non_climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_climate_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = pd.read_csv(DATA_PATH + 'non_procedural_hansard.csv')\n",
    "\n",
    "climate_hansard = topic_stems_filter(\n",
    "    non_procedural_hansard, climate_stems, 0.02)\n",
    "climate_hansard = climate_hansard.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_indices = climate_hansard.index\n",
    "non_climate_hansard = non_procedural_hansard.drop(climate_indices)\n",
    "\n",
    "climate_hansard.to_csv(DATA_PATH + 'climate_hansard.csv', index=False)\n",
    "non_climate_hansard.to_csv(DATA_PATH + 'non_climate_hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rest of the analysis is heavily dependent on what texts are filtered from Hansard and the Congressional Record, we need to validate how well the filtering process worked. This validation process involves the following steps:\n",
    "\n",
    "- Creating a labelled dataset of climate change speeches from Hansard and the Congressional Record.\n",
    "- Testing the performance of the climate change dictionary on the labelled dataset using the filtering rules applied above.\n",
    "- Optimising the filtering rules based on the climate change dictionary's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a labelled dataset of climate change speeches from Hansard and the Congressional Record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a labelled dataset, we first take a random sample of speecehs from Hansard made in 2008. Selecitng speeches from 2008 means that our sample is not representative of the entire Hansard corpus. However, given that 2008 was the year the UK Climate Change Act passed, it ensures we find some positive examples of climate change speeches. After making this random selection, we do the same for the Congressional Record, instead sampling speeches from 2009. 2009 was the year that the US Clean Energy and Security Act passed in the House of Representatives, but stalled in the Senate. Hence, we should find some positive examples of climate change speeches in this sample as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Hansard and the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = pd.read_csv(DATA_PATH + 'non_procedural_hansard.csv')\n",
    "non_procedural_congressional_record = pd.read_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly sampling 500 speeches from Hansard in 2008**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "hansard_2008 = non_procedural_hansard[non_procedural_hansard['year'] == 2008]\n",
    "hansard_2008_sample = hansard_2008.sample(500)\n",
    "\n",
    "hansard_2008_sample.to_csv(\n",
    "    DATA_PATH + 'hansard_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly sampling 500 speeches from the Congressional Record in 2009**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "congressional_record_2009 = non_procedural_congressional_record[\n",
    "    non_procedural_congressional_record['year'] == 2009]\n",
    "congressional_record_2009_sample = congressional_record_2009.sample(500)\n",
    "\n",
    "congressional_record_2009_sample.to_csv(\n",
    "    DATA_PATH + 'congressional_record_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, human coding is used to label the speeches in each sample as either climate change related or not. These annotations are saved as [classified_hansard_sample.csv](./data/classified_hansard_sample.csv) and [classified_congressional_record_sample.csv](./data/classified_congressional_record_sample.csv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dictionary performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test the performance of the climate change dictionary on the labelled datasets at different thresholds.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
