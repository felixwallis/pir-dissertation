{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code needed to filter the Congressional Record and Hansard datasets. The code creates three pairs of corpuses with cleaned and stemmed text. The first pair contains only speeches from the Congressional Record and Hansard that discuss issues relating to climate change. The second contains all the other non-climate change related speeches. The third pair contains only speeches from the Congressional Record and Hansard that discuss counter-terrorism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/5xzy77v17m1fbt0xq_sn3f140000gn/T/ipykernel_66200/3466160485.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "CONGRESSIONAL_RECORD_PATH = '../congressional-record/dist/'\n",
    "HANSARD_PATH = '../hansard-in-full/'\n",
    "CLIMATE_DICTIONARY_PATH = 'dictionaries/dist/'\n",
    "CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH = 'dictionaries/dist/'\n",
    "HANSARD_PROCEDURAL_STEMS_PATH = 'dictionaries/dist/'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "\n",
    "YEAR_RANGE = (1997, 2015)\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # Text should almost always be a string, but we check just in case\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stem the tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Rejoin the stemmed tokens\n",
    "    joined_stems = ' '.join(stemmed_tokens)\n",
    "    return joined_stems\n",
    "\n",
    "\n",
    "def corpus_preprocessing(corpus: pd.DataFrame,\n",
    "                         text_column_name: str,\n",
    "                         year_range: tuple,\n",
    "                         year_column_name: str = 'year'):\n",
    "    corpus = corpus.copy()\n",
    "    # Remove corpus content from outside the year range\n",
    "    corpus = corpus[corpus[year_column_name].between(\n",
    "        year_range[0], year_range[1])]\n",
    "    # Clean, tokenize, and stem the corpus\n",
    "    tqdm.pandas(desc=\"Processing Text\")\n",
    "    corpus['cleaned_stems'] = corpus[text_column_name].progress_apply(\n",
    "        tokenize_and_stem)\n",
    "    # Remove any documents with fewer than 10 stems\n",
    "    corpus['stem_count'] = corpus['cleaned_stems'].apply(\n",
    "        lambda x: len(x.split()))\n",
    "    corpus = corpus[corpus['stem_count'] >= 10]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PATH + 'congressional_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 1696694/1696694 [30:35<00:00, 924.62it/s] \n"
     ]
    }
   ],
   "source": [
    "congressional_record['date'] = pd.to_datetime(\n",
    "    congressional_record['date'], format='%Y%m%d')\n",
    "congressional_record['year'] = congressional_record['date'].dt.year\n",
    "\n",
    "congressional_record = corpus_preprocessing(\n",
    "    congressional_record, 'speech', YEAR_RANGE)\n",
    "\n",
    "congressional_record.to_csv(\n",
    "    DATA_PATH + 'congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(HANSARD_PATH + 'hansard_with_mp_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 1125378/1125378 [08:51<00:00, 2115.96it/s]\n"
     ]
    }
   ],
   "source": [
    "hansard['speech_date'] = pd.to_datetime(hansard['speech_date'])\n",
    "hansard['year'] = hansard['speech_date'].dt.year\n",
    "\n",
    "hansard = corpus_preprocessing(hansard, 'text', YEAR_RANGE)\n",
    "\n",
    "hansard.to_csv(DATA_PATH + 'hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(DATA_PATH + 'hansard.csv')\n",
    "congressional_record = pd.read_csv(DATA_PATH + 'congressional_record.csv')\n",
    "\n",
    "congressional_record_procedural_stems = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH + 'shortened_congressional_record_procedural_stems.csv')\n",
    "congressional_record_procedural_stems = set(\n",
    "    congressional_record_procedural_stems['stem'].tolist())\n",
    "hansard_procedural_stems = pd.read_csv(\n",
    "    HANSARD_PROCEDURAL_STEMS_PATH + 'shortened_hansard_procedural_stems.csv')\n",
    "hansard_procedural_stems = set(hansard_procedural_stems['stem'].tolist())\n",
    "\n",
    "climate_stems = pd.read_csv(\n",
    "    CLIMATE_DICTIONARY_PATH + 'climate_stems.csv')\n",
    "climate_stems = set(climate_stems['stem'].tolist())\n",
    "\n",
    "\n",
    "def term_proportion(doc, terms):\n",
    "    doc_tokens = set(doc.split())\n",
    "    term_count = sum(\n",
    "        term in doc_tokens for term in terms)\n",
    "    total_terms = len(doc_tokens)\n",
    "\n",
    "    if total_terms == 0:\n",
    "        return 0\n",
    "    return term_count / total_terms\n",
    "\n",
    "\n",
    "def procedural_stems_filter(corpus_df, procedural_stems, threshold: float = 0.5):\n",
    "    corpus_df = corpus_df.copy()\n",
    "    # Remove documents that contain more than than the threshold of procedural stems\n",
    "    corpus_df['procedural_proportion'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: term_proportion(doc, procedural_stems))\n",
    "    corpus_df = corpus_df[corpus_df['procedural_proportion'] < threshold]\n",
    "    # Remove all procedural stems from the remaining documents\n",
    "    corpus_df['cleaned_stems'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: ' '.join([word for word in doc.split() if word not in procedural_stems]))\n",
    "    return corpus_df\n",
    "\n",
    "\n",
    "def topic_stems_filter(corpus_df, topic_stems, threshold: float = 0.2):\n",
    "    corpus_df = corpus_df.copy()\n",
    "    # Remove any documents that contain fewer than the threshold proportion of topic stems\n",
    "    corpus_df['topic_proportion'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: term_proportion(doc, topic_stems))\n",
    "    corpus_df = corpus_df[corpus_df['topic_proportion'] > threshold]\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing procedural documents and stems from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = procedural_stems_filter(\n",
    "    congressional_record, congressional_record_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = procedural_stems_filter(\n",
    "    hansard, hansard_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_hansard.to_csv(\n",
    "    DATA_PATH + 'non_procedural_hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating climate change documents from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_congressional_record = topic_stems_filter(\n",
    "    non_procedural_congressional_record, climate_stems, 0.2)\n",
    "climate_congressional_record = climate_congressional_record.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_indices = climate_congressional_record.index\n",
    "non_climate_congressional_record = non_procedural_congressional_record.drop(\n",
    "    climate_indices)\n",
    "\n",
    "climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'climate_congressional_record.csv', index=False)\n",
    "non_climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_climate_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hansard = topic_stems_filter(\n",
    "    non_procedural_hansard, climate_stems, 0.2)\n",
    "climate_hansard = climate_hansard.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_indices = climate_hansard.index\n",
    "non_climate_hansard = non_procedural_hansard.drop(climate_indices)\n",
    "\n",
    "climate_hansard.to_csv(DATA_PATH + 'climate_hansard.csv', index=False)\n",
    "non_climate_hansard.to_csv(DATA_PATH + 'non_climate_hansard.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
