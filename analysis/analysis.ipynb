{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congressional Record and Hansard Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the analysis of the Congressional Record and Hansard datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/5xzy77v17m1fbt0xq_sn3f140000gn/T/ipykernel_52302/3461575144.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "congressional_record_path = '../congressional-record/dist/'\n",
    "hansard_path = '../hansard-in-full/'\n",
    "climate_dictionary_path = 'dictionaries/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing functions for the Congressional Record and Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    joined_tokens = ' '.join(filtered_tokens)\n",
    "    return joined_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning and tokenizing the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_csv(\n",
    "    congressional_record_path + 'individual_congresses/congress_111.csv')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "congressional_record['cleaned_tokens'] = congressional_record['speech'].apply(\n",
    "    clean_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing any documents with fewer than 10 words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record['token_count'] = congressional_record['cleaned_tokens'].apply(\n",
    "    lambda x: len(x.split()))\n",
    "\n",
    "congressional_record = congressional_record[congressional_record['token_count'] > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(hansard_path + 'hansard_with_mp_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Hansard content from before 1996 and after 2016**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speech_date'] = pd.to_datetime(hansard['speech_date'])\n",
    "hansard['year'] = hansard['speech_date'].dt.year\n",
    "\n",
    "hansard = hansard[(hansard['year'] >= 2009) & (hansard['year'] <= 2011)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning and tokenizing Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "hansard['cleaned_tokens'] = hansard['text'].apply(clean_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing any documents with fewer than 10 words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['token_count'] = hansard['cleaned_tokens'].apply(\n",
    "    lambda x: len(x.split()))\n",
    "\n",
    "hansard = hansard[hansard['token_count'] > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering functions for the Congressional Record and Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(corpus, ngram_range):\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=None, preprocessor=None, ngram_range=ngram_range)\n",
    "    count_matrix = vectorizer.fit_transform(corpus)\n",
    "    word_sums = count_matrix.sum(axis=0)\n",
    "    word_sums = np.array(word_sums).flatten()\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_freq = zip(feature_names, word_sums)\n",
    "    sorted_word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_word_freq\n",
    "\n",
    "\n",
    "def procedural_proportion(doc, top_trigrams):\n",
    "    doc_trigrams = list(nltk.trigrams(doc.split()))\n",
    "    doc_trigrams = [' '.join(trigram) for trigram in doc_trigrams]\n",
    "    procedural_count = sum(trigram in top_trigrams for trigram in doc_trigrams)\n",
    "    if len(doc_trigrams) == 0:\n",
    "        return 0\n",
    "    return procedural_count / len(doc_trigrams)\n",
    "\n",
    "\n",
    "def contains_climate_term(doc):\n",
    "    doc_tokens = set(doc.split())\n",
    "    climate_term_count = sum(term in doc_tokens for term in climate_terms)\n",
    "    total_terms = len(doc_tokens)\n",
    "\n",
    "    if total_terms == 0:\n",
    "        return 0\n",
    "    return climate_term_count / total_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the 100 most frequently used trigrams in the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = get_ngrams(congressional_record['cleaned_tokens'], (3, 3))\n",
    "trigrams_df = pd.DataFrame(trigrams, columns=['trigram', 'frequency'])\n",
    "top_100_trigrams = trigrams_df['trigram'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing documents from the Congressional Record with over 20% of trirgams that match the 100 most frequently used trigrams**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_trigrams_list = top_100_trigrams.tolist()\n",
    "congressional_record['procedural_proportion'] = congressional_record['cleaned_tokens'].apply(\n",
    "    lambda doc: procedural_proportion(doc, top_100_trigrams_list))\n",
    "\n",
    "congressional_record = congressional_record[congressional_record['procedural_proportion'] < 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting documents that discuss climate change**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_dictionary = pd.read_csv(\n",
    "    climate_dictionary_path + 'cleaned_climate_dictionary.csv')\n",
    "\n",
    "climate_terms = set(climate_dictionary['term'].str.lower().tolist())\n",
    "\n",
    "congressional_record['climate_term_proportion'] = congressional_record['cleaned_tokens'].apply(\n",
    "    contains_climate_term)\n",
    "congressional_record = congressional_record.sort_values(\n",
    "    'climate_term_proportion', ascending=False)\n",
    "\n",
    "climate_congressional_record = congressional_record[\n",
    "    congressional_record['climate_term_proportion'] > 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the 100 most frequently used trigrams in Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = get_ngrams(hansard['cleaned_tokens'], (3, 3))\n",
    "trigrams_df = pd.DataFrame(trigrams, columns=['trigram', 'frequency'])\n",
    "top_100_trigrams = trigrams_df['trigram'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing documents from Hansard with over 20% of trirgams that match the 100 most frequently used trigrams**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_trigrams_list = top_100_trigrams.tolist()\n",
    "hansard['procedural_proportion'] = hansard['cleaned_tokens'].apply(\n",
    "    lambda doc: procedural_proportion(doc, top_100_trigrams_list))\n",
    "\n",
    "hansard = hansard[hansard['procedural_proportion'] < 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting documents that discuss climate change**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_dictionary = pd.read_csv(\n",
    "    climate_dictionary_path + 'cleaned_climate_dictionary.csv')\n",
    "\n",
    "climate_terms = set(climate_dictionary['term'].str.lower().tolist())\n",
    "\n",
    "hansard['climate_term_proportion'] = hansard['cleaned_tokens'].apply(\n",
    "    contains_climate_term)\n",
    "hansard = hansard.sort_values(\n",
    "    'climate_term_proportion', ascending=False)\n",
    "\n",
    "climate_hansard = hansard[hansard['climate_term_proportion'] > 0.01]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
