{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3adc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0ddcb",
   "metadata": {},
   "source": [
    "# Hansard scraper from TheyWorkForYou archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dccbe",
   "metadata": {},
   "source": [
    "See [Ludovic Rheault's GitHub](https://github.com/lrheault/emotion/blob/master/webscrape_hansard.py) for example code.\n",
    "\n",
    "We want to scrape files from [TheyWorkForYou](www.theyworkforyou.com), where debates are stored in XML format with individual URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68a7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_debate_urls():\n",
    "\n",
    "    # load requests module to look-up www.theyworkforyou.com\n",
    "    import requests\n",
    "    # And BeatifulSoup to parse the resulting XML\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Get response from URL and decode\n",
    "    url = \"https://www.theyworkforyou.com/pwdata/scrapedxml/debates/\"\n",
    "    response = requests.get(url)\n",
    "    decoded_response = response.content.decode()\n",
    "\n",
    "    # Tidy with BeatifulSoup\n",
    "    bs = BeautifulSoup(decoded_response)\n",
    "\n",
    "    # Find instances of 'a' tag\n",
    "    tags = bs.find_all('a')\n",
    "    debate_urls = []\n",
    "    for link in tags:\n",
    "        if '.xml' in link['href']:  # We only want .xml links - not .txt or others\n",
    "            debate_urls.append(url+link['href'])\n",
    "\n",
    "    return debate_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24d3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_urls = get_list_of_debate_urls()\n",
    "pd.Series(debate_urls, name='url').to_csv('debate_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83399290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def save_debate_xml_to_disk(url, filename=None):\n",
    "    # Read and decode URL\n",
    "    response = requests.get(url)\n",
    "    # Where we have a decoding error, replace the tricky byte with '?'\n",
    "    decoded_response = response.content.decode(errors='replace')\n",
    "    encoded_response = decoded_response.encode('utf-8')\n",
    "\n",
    "    # Set filename to write to disk\n",
    "    if filename is None:\n",
    "        filename = 'debates_xml/'+url.split('/')[-1]\n",
    "\n",
    "    # Check if directory exists, if not, create it\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(encoded_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cccfa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18877/18877 [1:03:00<00:00,  4.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(debate_urls):\n",
    "    save_debate_xml_to_disk(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
