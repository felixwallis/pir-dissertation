{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Procedural Stems Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes the [online index of Erskine May]('https://erskinemay.parliament.uk/browse/indexterms?page=1') to create a list of procedural stems used in the UK Parliament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "HANSARD_PATH = '../../hansard-in-full/'\n",
    "\n",
    "YEAR_RANGE = (1997, 2015)\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting parliamentary procedural terms from the online index of Erskine May\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting terms from 23.html...\n",
      "Extracting terms from 35.html...\n",
      "Extracting terms from 9.html...\n",
      "Extracting terms from 19.html...\n",
      "Extracting terms from 39.html...\n",
      "Extracting terms from 5.html...\n",
      "Extracting terms from 15.html...\n",
      "Extracting terms from 42.html...\n",
      "Extracting terms from 54.html...\n",
      "Extracting terms from 43.html...\n",
      "Extracting terms from 14.html...\n",
      "Extracting terms from 4.html...\n",
      "Extracting terms from 38.html...\n",
      "Extracting terms from 18.html...\n",
      "Extracting terms from 8.html...\n",
      "Extracting terms from 34.html...\n",
      "Extracting terms from 22.html...\n",
      "Extracting terms from 29.html...\n",
      "Extracting terms from 3.html...\n",
      "Extracting terms from 13.html...\n",
      "Extracting terms from 44.html...\n",
      "Extracting terms from 52.html...\n",
      "Extracting terms from 25.html...\n",
      "Extracting terms from 33.html...\n",
      "Extracting terms from 48.html...\n",
      "Extracting terms from 49.html...\n",
      "Extracting terms from 32.html...\n",
      "Extracting terms from 24.html...\n",
      "Extracting terms from 53.html...\n",
      "Extracting terms from 45.html...\n",
      "Extracting terms from 12.html...\n",
      "Extracting terms from 2.html...\n",
      "Extracting terms from 28.html...\n",
      "Extracting terms from 50.html...\n",
      "Extracting terms from 46.html...\n",
      "Extracting terms from 11.html...\n",
      "Extracting terms from 1.html...\n",
      "Extracting terms from 31.html...\n",
      "Extracting terms from 27.html...\n",
      "Extracting terms from 26.html...\n",
      "Extracting terms from 30.html...\n",
      "Extracting terms from 10.html...\n",
      "Extracting terms from 47.html...\n",
      "Extracting terms from 51.html...\n",
      "Extracting terms from 37.html...\n",
      "Extracting terms from 21.html...\n",
      "Extracting terms from 40.html...\n",
      "Extracting terms from 17.html...\n",
      "Extracting terms from 7.html...\n",
      "Extracting terms from 6.html...\n",
      "Extracting terms from 16.html...\n",
      "Extracting terms from 41.html...\n",
      "Extracting terms from 20.html...\n",
      "Extracting terms from 36.html...\n"
     ]
    }
   ],
   "source": [
    "def extract_terms(html_file_path, filename):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "\n",
    "    index_terms = soup.find_all('span', class_='text')\n",
    "    return [(term.get_text(strip=True), filename) for term in index_terms]\n",
    "\n",
    "\n",
    "def extract_terms_from_files(directory):\n",
    "    terms = []\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".html\"):\n",
    "            print(f'Extracting terms from {filename}...')\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            terms.extend(extract_terms(file_path, filename))\n",
    "    return terms\n",
    "\n",
    "\n",
    "directory = DATA_PATH + 'erskine-may-index/'\n",
    "index_terms = extract_terms_from_files(directory)\n",
    "index_terms_df = pd.DataFrame(index_terms, columns=['term', 'source_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning the procedural terms into a dictionary of stemmed unique unigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for the procedural terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    # Text should almost always be a string, but we check\n",
    "    # just in case\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and stemming the procedural terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms_df['cleaned_term'] = index_terms_df['term'].apply(clean_tokenize)\n",
    "exploded_index_terms_df = index_terms_df.explode('cleaned_term')\n",
    "unique_terms = exploded_index_terms_df['cleaned_term'].unique()\n",
    "unique_terms_df = pd.DataFrame(\n",
    "    unique_terms, columns=['stem']).sort_values(by='stem').reset_index(drop=True)\n",
    "\n",
    "unique_terms_df.to_csv(\n",
    "    DATA_PATH + 'hansard_procedural_stems.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is manually cleaned at this point to create the final [`shortened_hansard_procedural_terms.csv` file](https://docs.google.com/spreadsheets/d/1twVZ_ypcBOLroMDxgbC0veFKvHq7BbT9HbW99zUnNU8/edit?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_terms_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding stems that are frequently used by the Speaker of the House of Commons to the dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions for Procedural Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # Text should almost always be a string, but we check just in case\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stem the tokens\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Rejoin the stemmed tokens\n",
    "    joined_stems = ' '.join(stemmed_tokens)\n",
    "    return joined_stems\n",
    "\n",
    "\n",
    "def corpus_preprocessing(corpus: pd.DataFrame,\n",
    "                         text_column_name: str,\n",
    "                         year_range: tuple,\n",
    "                         year_column_name: str = 'year'):\n",
    "    corpus = corpus.copy()\n",
    "    # Remove corpus content from outside the year range\n",
    "    corpus = corpus[corpus[year_column_name].between(\n",
    "        year_range[0], year_range[1])]\n",
    "    # Clean, tokenize, and stem the corpus\n",
    "    tqdm.pandas(desc=\"Processing Text\")\n",
    "    corpus['cleaned_stems'] = corpus[text_column_name].progress_apply(\n",
    "        tokenize_and_stem)\n",
    "    # Remove any documents with fewer than 10 stems\n",
    "    corpus['stem_count'] = corpus['cleaned_stems'].apply(\n",
    "        lambda x: len(x.split()))\n",
    "    corpus = corpus[corpus['stem_count'] >= 10]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedural Hansard preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(HANSARD_PATH + 'hansard_with_mp_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Procedural Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|██████████| 15242/15242 [00:02<00:00, 5694.51it/s] \n"
     ]
    }
   ],
   "source": [
    "hansard['speech_date'] = pd.to_datetime(hansard['speech_date'])\n",
    "hansard['year'] = hansard['speech_date'].dt.year\n",
    "\n",
    "procedural_hansard = hansard[hansard['memberships'].isna()]\n",
    "procedural_hansard = corpus_preprocessing(\n",
    "    procedural_hansard, 'text', YEAR_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most frequently used stems in the Procedural Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turning the procedural Hansard into a document-feature matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "procedural_hansard_dfm = vectorizer.fit_transform(\n",
    "    procedural_hansard['cleaned_stems'])\n",
    "procedural_hansard_dfm_df = pd.DataFrame(\n",
    "    procedural_hansard_dfm.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the most frequently used stems**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hon       3288\n",
       "member    2214\n",
       "amend     1933\n",
       "would     1730\n",
       "line      1580\n",
       "          ... \n",
       "believ     330\n",
       "polic      322\n",
       "place      321\n",
       "deal       320\n",
       "like       318\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_counts = procedural_hansard_dfm_df.sum().sort_values(ascending=False)\n",
    "top_stems = stem_counts.head(100)\n",
    "top_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the most frequently used stems to the procedural stems dictionary if they have not already been added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_procedural_stems = pd.read_csv(\n",
    "    DIST_PATH + 'shortened_hansard_procedural_stems.csv')\n",
    "hansard_procedural_stems = set(hansard_procedural_stems['stem'].tolist())\n",
    "\n",
    "top_stems_set = set(top_stems.index)\n",
    "missing_stems = top_stems_set.difference(hansard_procedural_stems)\n",
    "\n",
    "hansard_procedural_stems = hansard_procedural_stems.union(missing_stems)\n",
    "hansard_procedural_stems_df = pd.DataFrame(\n",
    "    list(hansard_procedural_stems), columns=['stem'])\n",
    "hansard_procedural_stems_df.to_csv(\n",
    "    DIST_PATH + 'expanded_hansard_procedural_stems.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
