{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congressional Record and Hansard Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the analysis of the Congressional Record and Hansard datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/5xzy77v17m1fbt0xq_sn3f140000gn/T/ipykernel_49799/3681674360.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "congressional_record_path = '../congressional-record/dist/'\n",
    "climate_dictionary_path = 'dist/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_csv(\n",
    "    congressional_record_path + 'individual_congresses/congress_111.csv')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Clean and tokenize text\n",
    "def clean_tokenize(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    joined_tokens = ' '.join(filtered_tokens)\n",
    "    return joined_tokens\n",
    "\n",
    "\n",
    "congressional_record['cleaned_tokens'] = congressional_record['speech'].apply(\n",
    "    clean_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove any documents that are less than 10 words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record['token_count'] = congressional_record['cleaned_tokens'].apply(\n",
    "    lambda x: len(x.split()))\n",
    "\n",
    "congressional_record = congressional_record[congressional_record['token_count'] > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Returning n-gram frequencies from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(corpus, ngram_range):\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=None, preprocessor=None, ngram_range=ngram_range)\n",
    "    count_matrix = vectorizer.fit_transform(corpus)\n",
    "    word_sums = count_matrix.sum(axis=0)\n",
    "    word_sums = np.array(word_sums).flatten()\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_freq = zip(feature_names, word_sums)\n",
    "    sorted_word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the 100 most frequently used trigrams in the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = get_ngrams(congressional_record['cleaned_tokens'], (3, 3))\n",
    "trigrams_df = pd.DataFrame(trigrams, columns=['trigram', 'frequency'])\n",
    "top_100_trigrams = trigrams_df['trigram'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing documents from the Congressional Record with over 20% of trirgams that match the 100 most frequently used trigrams**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedural_proportion(doc, top_trigrams):\n",
    "    doc_trigrams = list(nltk.trigrams(doc.split()))\n",
    "    doc_trigrams = [' '.join(trigram) for trigram in doc_trigrams]\n",
    "    procedural_count = sum(trigram in top_trigrams for trigram in doc_trigrams)\n",
    "    if len(doc_trigrams) == 0:\n",
    "        return 0\n",
    "    return procedural_count / len(doc_trigrams)\n",
    "\n",
    "\n",
    "top_100_trigrams_list = top_100_trigrams.tolist()\n",
    "congressional_record['procedural_proportion'] = congressional_record['cleaned_tokens'].apply(\n",
    "    lambda doc: procedural_proportion(doc, top_100_trigrams_list))\n",
    "\n",
    "congressional_record = congressional_record[congressional_record['procedural_proportion'] < 0.2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
