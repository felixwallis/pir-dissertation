{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code needed to filter the Congressional Record and Hansard datasets. The code creates two pairs of corpuses with cleaned and stemmed text. The first pair contains only speeches from the Congressional Record and Hansard that discuss issues relating to climate change. The second contains all speeches made between 1997 and 2015 for each corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ssl\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "CONGRESSIONAL_RECORD_PATH = '../../congressional-record/dist/'\n",
    "HANSARD_PATH = '../../hansard-in-full/'\n",
    "CLIMATE_DICTIONARY_PATH = '../dictionaries/dist/'\n",
    "CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH = '../dictionaries/dist/'\n",
    "HANSARD_PROCEDURAL_STEMS_PATH = '../dictionaries/dist/'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "\n",
    "YEAR_RANGE = (2009, 2010)\n",
    "\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # Convert text to lowercase and remove non-alphabetic characters\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text and remove stop words\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stem the tokens and store each stem with its original token\n",
    "    stemmed_tokens_with_original = [\n",
    "        (token, stemmer.stem(token)) for token in filtered_tokens]\n",
    "    return stemmed_tokens_with_original\n",
    "\n",
    "\n",
    "def corpus_cleaning_tokenizing_stemming(corpus: pd.DataFrame, text_column_name: str, year: int):\n",
    "    corpus = corpus.copy()\n",
    "\n",
    "    # Filter the corpus for the specific year\n",
    "    corpus = corpus[corpus['year'] == year]\n",
    "\n",
    "    # Clean, tokenize, and stem the corpus for the specific year\n",
    "    tqdm.pandas(desc=f\"Processing Text for Year {year}\")\n",
    "    corpus['cleaned_stems_with_original'] = corpus[text_column_name].progress_apply(\n",
    "        tokenize_and_stem)\n",
    "\n",
    "    # Remove any documents with fewer than 50 stems\n",
    "    corpus['stem_count'] = corpus['cleaned_stems_with_original'].apply(len)\n",
    "    corpus = corpus[corpus['stem_count'] >= 50]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def process_dataframe(df, text_column_name, year_range, data_path):\n",
    "    # Create a temporary directory to store the yearly dataframes\n",
    "    temp_dir = os.path.join(data_path, \"temp_yearly_dataframes\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Process and save the dataframe for each year in the year range\n",
    "    for year in range(year_range[0], year_range[1] + 1):\n",
    "        yearly_df = corpus_cleaning_tokenizing_stemming(\n",
    "            df, text_column_name, year)\n",
    "        yearly_df.to_pickle(os.path.join(\n",
    "            temp_dir, f\"congressional_record_{year}.pkl\"))\n",
    "\n",
    "    yearly_dataframes = []\n",
    "\n",
    "    # Load the processed yearly dataframes from the temporary directory\n",
    "    for year in range(year_range[0], year_range[1] + 1):\n",
    "        yearly_df = pd.read_pickle(os.path.join(\n",
    "            temp_dir, f\"congressional_record_{year}.pkl\"))\n",
    "        yearly_dataframes.append(yearly_df)\n",
    "\n",
    "    # Remove the temporary directory and its contents\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    # Concatenate the yearly dataframes into a single processed dataframe\n",
    "    processed_df = pd.concat(yearly_dataframes, ignore_index=True)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def corpus_filtering(corpus: pd.DataFrame, min_df: int = 10):\n",
    "    # Create a set of allowed stems based on the min_df threshold\n",
    "    vectorizer = CountVectorizer(min_df=min_df)\n",
    "    vectorizer.fit_transform(corpus['cleaned_stems_with_original'].apply(\n",
    "        lambda x: ' '.join([stem for _, stem in x])))\n",
    "    allowed_stems = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Filter out stems that occur in less than min_df documents\n",
    "    tqdm.pandas(desc=\"Filtering Stems\")\n",
    "    corpus['cleaned_stems_with_original'] = corpus['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda x: [(token, stem) for token, stem in x if stem in allowed_stems])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PATH + 'congressional_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text for Year 2009: 100%|██████████| 47428/47428 [01:06<00:00, 708.59it/s] \n",
      "Processing Text for Year 2010: 100%|██████████| 47156/47156 [01:10<00:00, 664.49it/s] \n"
     ]
    }
   ],
   "source": [
    "congressional_record['date'] = pd.to_datetime(\n",
    "    congressional_record['date'], format='%Y%m%d')\n",
    "congressional_record['year'] = congressional_record['date'].dt.year\n",
    "\n",
    "# Remove any speeches with missing speaker_id\n",
    "congressional_record = congressional_record.dropna(subset=['speaker_id'])\n",
    "\n",
    "# Process the dataframe year by year and concatenate the results\n",
    "congressional_record = process_dataframe(\n",
    "    congressional_record, 'speech', YEAR_RANGE, DATA_PATH)\n",
    "congressional_record.to_pickle(\n",
    "    DATA_PATH + 'congressional_record.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Stems: 100%|██████████| 42632/42632 [00:01<00:00, 24692.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "congressional_record = pd.read_pickle(DATA_PATH + 'congressional_record.pkl')\n",
    "congressional_record = corpus_filtering(congressional_record)\n",
    "congressional_record.to_pickle(DATA_PATH + 'congressional_record.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_csv(HANSARD_PATH + 'hansard_with_mp_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text for Year 2009: 100%|██████████| 61895/61895 [00:26<00:00, 2368.36it/s]\n",
      "Processing Text for Year 2010: 100%|██████████| 62744/62744 [00:26<00:00, 2382.61it/s]\n"
     ]
    }
   ],
   "source": [
    "hansard['speech_date'] = pd.to_datetime(hansard['speech_date'])\n",
    "hansard['year'] = hansard['speech_date'].dt.year\n",
    "\n",
    "# Remove any speeches with missing memberships\n",
    "hansard = hansard.dropna(subset=['memberships'])\n",
    "\n",
    "# Cleaning, tokenizing, and stemming\n",
    "hansard = process_dataframe(hansard, 'text', YEAR_RANGE, DATA_PATH)\n",
    "hansard.to_pickle(DATA_PATH + 'hansard.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Stems: 100%|██████████| 20326/20326 [00:02<00:00, 9201.86it/s] \n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "hansard = pd.read_pickle(DATA_PATH + 'hansard.pkl')\n",
    "hansard = corpus_filtering(hansard)\n",
    "hansard.to_pickle(DATA_PATH + 'hansard.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering functions for the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_proportion(doc_stems, terms):\n",
    "    doc_tokens = set(doc_stems)\n",
    "    term_count = sum(term in doc_tokens for term in terms)\n",
    "    total_terms = len(doc_tokens)\n",
    "    if total_terms == 0:\n",
    "        return 0\n",
    "    return term_count / total_terms\n",
    "\n",
    "\n",
    "def procedural_stems_filter(corpus_df, procedural_stems, threshold: float = 0.5):\n",
    "    corpus_df = corpus_df.copy()\n",
    "\n",
    "    # Remove documents that contain more than the threshold of procedural stems\n",
    "    tqdm.pandas(desc=\"Calculating procedural proportion\")\n",
    "    corpus_df['procedural_proportion'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: term_proportion([stem for _, stem in doc], procedural_stems))\n",
    "    corpus_df = corpus_df[corpus_df['procedural_proportion'] < threshold]\n",
    "\n",
    "    # Remove all procedural stems from the remaining documents\n",
    "    tqdm.pandas(desc=\"Removing procedural stems\")\n",
    "    corpus_df['cleaned_stems_with_original'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: [(token, stem) for token, stem in doc if stem not in procedural_stems])\n",
    "\n",
    "    # Join the stems and original tokens back into separate strings\n",
    "    tqdm.pandas(desc=\"Joining cleaned stems\")\n",
    "    corpus_df['cleaned_stems'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: ' '.join([stem for _, stem in doc]))\n",
    "\n",
    "    tqdm.pandas(desc=\"Joining cleaned tokens\")\n",
    "    corpus_df['cleaned_tokens'] = corpus_df['cleaned_stems_with_original'].progress_apply(\n",
    "        lambda doc: ' '.join([token for token, _ in doc]))\n",
    "\n",
    "    corpus_df = corpus_df.drop(columns=['cleaned_stems_with_original'])\n",
    "\n",
    "    return corpus_df\n",
    "\n",
    "\n",
    "def topic_stems_filter(corpus_df, topic_stems, threshold: float = 0.2):\n",
    "    corpus_df = corpus_df.copy()\n",
    "\n",
    "    # Remove any documents that contain fewer than the threshold proportion of topic stems\n",
    "    corpus_df['topic_proportion'] = corpus_df['cleaned_stems'].apply(\n",
    "        lambda doc: term_proportion(doc.split(), topic_stems)\n",
    "    )\n",
    "    corpus_df = corpus_df[corpus_df['topic_proportion'] > threshold]\n",
    "\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing procedural documents and stems from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the necessary data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_record = pd.read_pickle(DATA_PATH + 'congressional_record.pkl')\n",
    "hansard = pd.read_pickle(DATA_PATH + 'hansard.pkl')\n",
    "\n",
    "congressional_record_procedural_stems = pd.read_csv(\n",
    "    CONGRESSIONAL_RECORD_PROCEDURAL_STEMS_PATH + 'shortened_congressional_record_procedural_stems.csv')\n",
    "congressional_record_procedural_stems = set(\n",
    "    congressional_record_procedural_stems['stem'].tolist())\n",
    "\n",
    "hansard_procedural_stems = pd.read_csv(\n",
    "    HANSARD_PROCEDURAL_STEMS_PATH + 'expanded_hansard_procedural_stems.csv')\n",
    "hansard_procedural_stems = set(hansard_procedural_stems['stem'].tolist())\n",
    "\n",
    "climate_stems = pd.read_csv(\n",
    "    CLIMATE_DICTIONARY_PATH + 'shortened_climate_stems.csv')\n",
    "climate_stems = set(climate_stems['stem'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = procedural_stems_filter(\n",
    "    congressional_record, congressional_record_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing procedural documents and stems from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = procedural_stems_filter(\n",
    "    hansard, hansard_procedural_stems, 0.5)\n",
    "\n",
    "non_procedural_hansard.to_csv(\n",
    "    DATA_PATH + 'non_procedural_hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating climate change documents from the Congressional Record and Hansard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from the Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = pd.read_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv')\n",
    "\n",
    "climate_congressional_record = topic_stems_filter(\n",
    "    non_procedural_congressional_record, climate_stems, 0.02)\n",
    "climate_congressional_record = climate_congressional_record.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_indices = climate_congressional_record.index\n",
    "non_climate_congressional_record = non_procedural_congressional_record.drop(\n",
    "    climate_indices)\n",
    "\n",
    "climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'climate_congressional_record.csv', index=False)\n",
    "non_climate_congressional_record.to_csv(\n",
    "    DATA_PATH + 'non_climate_congressional_record.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating climate change documents from Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = pd.read_csv(DATA_PATH + 'non_procedural_hansard.csv')\n",
    "\n",
    "climate_hansard = topic_stems_filter(\n",
    "    non_procedural_hansard, climate_stems, 0.025)\n",
    "climate_hansard = climate_hansard.sort_values(\n",
    "    'topic_proportion', ascending=False)\n",
    "\n",
    "climate_indices = climate_hansard.index\n",
    "non_climate_hansard = non_procedural_hansard.drop(climate_indices)\n",
    "\n",
    "climate_hansard.to_csv(DATA_PATH + 'climate_hansard.csv', index=False)\n",
    "non_climate_hansard.to_csv(DATA_PATH + 'non_climate_hansard.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rest of the analysis is heavily dependent on what texts are filtered from Hansard and the Congressional Record, we need to validate how well the filtering process worked. This validation process involves the following steps:\n",
    "\n",
    "- Creating a labelled dataset of climate change speeches from Hansard and the Congressional Record.\n",
    "- Testing the performance of the climate change dictionary on the labelled dataset using the filtering rules applied above.\n",
    "- Optimising the filtering rules based on the climate change dictionary's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a labelled dataset of climate change speeches from Hansard and the Congressional Record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a labelled dataset, we first take a random sample of speecehs from Hansard made in 2008. Selecitng speeches from 2008 means that our sample is not representative of the entire Hansard corpus. However, given that 2008 was the year the UK Climate Change Act passed, it ensures we find some positive examples of climate change speeches. After making this random selection, we do the same for the Congressional Record, instead sampling speeches from 2009. 2009 was the year that the US Clean Energy and Security Act passed in the House of Representatives, but stalled in the Senate. Hence, we should find some positive examples of climate change speeches in this sample as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Congressional Record and Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = pd.read_csv(DATA_PATH + 'non_procedural_hansard.csv')\n",
    "non_procedural_congressional_record = pd.read_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly sampling 500 speeches from the Congressional Record in 2009**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "congressional_record_2009 = non_procedural_congressional_record[\n",
    "    non_procedural_congressional_record['year'] == 2009]\n",
    "congressional_record_2009_sample = congressional_record_2009.sample(500)\n",
    "\n",
    "congressional_record_2009_sample.to_csv(\n",
    "    DATA_PATH + 'congressional_record_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly sampling 500 speeches from Hansard in 2008**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "hansard_2008 = non_procedural_hansard[non_procedural_hansard['year'] == 2008]\n",
    "hansard_2008_sample = hansard_2008.sample(500)\n",
    "\n",
    "hansard_2008_sample.to_csv(\n",
    "    DATA_PATH + 'hansard_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, human coding is used to label the speeches in each sample as either climate change related or not. These annotations are saved as [classified_hansard_sample.csv](./data/classified_hansard_sample.csv) and [classified_congressional_record_sample.csv](./data/classified_congressional_record_sample.csv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dictionary performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test the performance of the climate change dictionary on the labelled datasets at different thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_congressional_record_sample = pd.read_csv(\n",
    "    DATA_PATH + 'classified_congressional_record_sample.csv')\n",
    "labels = classified_congressional_record_sample['climate_change_content']\n",
    "\n",
    "start_threshold = 0.005\n",
    "end_threshold = 0.07\n",
    "step = 0.002\n",
    "\n",
    "thresholds = np.arange(start_threshold, end_threshold + step, step)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_climate_change_content = topic_stems_filter(\n",
    "        classified_congressional_record_sample, climate_stems, threshold)\n",
    "    classified_congressional_record_sample['predicted_climate_change_content'] = classified_congressional_record_sample.index.isin(\n",
    "        predicted_climate_change_content.index)\n",
    "    predicted_labels = classified_congressional_record_sample['predicted_climate_change_content']\n",
    "\n",
    "    precision = precision_score(\n",
    "        labels, predicted_labels, pos_label=True)\n",
    "    recall = recall_score(labels, predicted_labels, pos_label=True)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "# Plot precision and recall scores against thresholds\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precision_scores, marker='o', label='Precision')\n",
    "plt.plot(thresholds, recall_scores, marker='o', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Congressional Record: Precision and Recall vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_hansard_sample = pd.read_csv(\n",
    "    DATA_PATH + 'classified_hansard_sample.csv')\n",
    "labels = classified_hansard_sample['climate_change_content']\n",
    "\n",
    "start_threshold = 0.005\n",
    "end_threshold = 0.07\n",
    "step = 0.002\n",
    "\n",
    "thresholds = np.arange(start_threshold, end_threshold + step, step)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_climate_change_content = topic_stems_filter(\n",
    "        classified_hansard_sample, climate_stems, threshold)\n",
    "    classified_hansard_sample['predicted_climate_change_content'] = classified_hansard_sample.index.isin(\n",
    "        predicted_climate_change_content.index)\n",
    "    predicted_labels = classified_hansard_sample['predicted_climate_change_content']\n",
    "\n",
    "    precision = precision_score(\n",
    "        labels, predicted_labels, pos_label=True)\n",
    "    recall = recall_score(labels, predicted_labels, pos_label=True)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "# Plot precision and recall scores against thresholds\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precision_scores, marker='o', label='Precision')\n",
    "plt.plot(thresholds, recall_scores, marker='o', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Hansard: Precision and Recall vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing party balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to test how the filtering process affects the balance of parties in the filtered datasets. The threshold should not have a statistically significant effect on this balance. Otherwise, the following results are likely being driven by the filtering process rather than the content of the speeches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congressional Record**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_congressional_record = pd.read_csv(\n",
    "    DATA_PATH + 'non_procedural_congressional_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the difference in the proportion of speeches made by Democrats and Republicans\n",
    "# in the non-procedural Congressional Record\n",
    "republican_speeches = non_procedural_congressional_record[\n",
    "    non_procedural_congressional_record['party'] == 'R']\n",
    "democratic_speeches = non_procedural_congressional_record[\n",
    "    non_procedural_congressional_record['party'] == 'D']\n",
    "\n",
    "prop_republican_speeches = len(\n",
    "    republican_speeches) / len(non_procedural_congressional_record)\n",
    "prop_democratic_speeches = len(\n",
    "    democratic_speeches) / len(non_procedural_congressional_record)\n",
    "diff_prop_speeches = prop_democratic_speeches - prop_republican_speeches\n",
    "\n",
    "# Calculating the difference in the proportion of speeches about climate change\n",
    "# made by Democrats and Republicans in the non-procedural Congressional Record\n",
    "# at different thresholds\n",
    "start_threshold = 0.005\n",
    "end_threshold = 0.07\n",
    "step = 0.002\n",
    "thresholds = np.arange(start_threshold, end_threshold + step, step)\n",
    "\n",
    "diff_prop_climate_speeches_list = []\n",
    "\n",
    "for threshold in tqdm(thresholds, desc=\"Processing thresholds\"):\n",
    "    climate_congressional_record = topic_stems_filter(\n",
    "        non_procedural_congressional_record, climate_stems, threshold)\n",
    "\n",
    "    republican_climate_speeches = climate_congressional_record[\n",
    "        climate_congressional_record['party'] == 'R']\n",
    "    democratic_climate_speeches = climate_congressional_record[\n",
    "        climate_congressional_record['party'] == 'D']\n",
    "\n",
    "    prop_republican_climate_speeches = len(\n",
    "        republican_climate_speeches) / len(climate_congressional_record)\n",
    "    prop_democratic_climate_speeches = len(\n",
    "        democratic_climate_speeches) / len(climate_congressional_record)\n",
    "\n",
    "    diff_prop_climate_speeches = prop_democratic_climate_speeches - \\\n",
    "        prop_republican_climate_speeches\n",
    "    diff_prop_climate_speeches_list.append(diff_prop_climate_speeches)\n",
    "\n",
    "# Plotting the difference in proportion of speeches about climate change against thresholds\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, diff_prop_climate_speeches_list,\n",
    "         marker='o', label='Difference in Proportion')\n",
    "plt.axhline(y=diff_prop_speeches, color='b', linestyle='--',\n",
    "            label='Reference in all Speeches')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Difference in Proportion')\n",
    "plt.title(\n",
    "    'Congressional Record: Difference in Proportion of Speeches about Climate Change')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hansard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_procedural_hansard = pd.read_csv(DATA_PATH + 'non_procedural_hansard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the difference in the proportion of speeches made by Conservative and Labour\n",
    "# MPs in the non-procedural Hansard\n",
    "conservative_speeches = non_procedural_hansard[\n",
    "    non_procedural_hansard['speech_party'] == 'Conservative']\n",
    "labour_speeches = non_procedural_hansard[\n",
    "    non_procedural_hansard['speech_party'] == 'Labour']\n",
    "\n",
    "prop_conservative_speeches = len(\n",
    "    conservative_speeches) / len(non_procedural_hansard)\n",
    "prop_labour_speeches = len(labour_speeches) / len(non_procedural_hansard)\n",
    "diff_prop_speeches = prop_labour_speeches - prop_conservative_speeches\n",
    "\n",
    "# Calculating the difference in the proportion of speeches about climate change\n",
    "# made by Conservative and Labour MPs in the non-procedural Hansard at different thresholds\n",
    "start_threshold = 0.005\n",
    "end_threshold = 0.07\n",
    "step = 0.002\n",
    "\n",
    "thresholds = np.arange(start_threshold, end_threshold + step, step)\n",
    "diff_prop_climate_speeches_list = []\n",
    "\n",
    "for threshold in tqdm(thresholds, desc=\"Processing thresholds\"):\n",
    "    climate_hansard = topic_stems_filter(\n",
    "        non_procedural_hansard, climate_stems, threshold)\n",
    "\n",
    "    conservative_climate_speeches = climate_hansard[\n",
    "        climate_hansard['speech_party'] == 'Conservative']\n",
    "    labour_climate_speeches = climate_hansard[\n",
    "        climate_hansard['speech_party'] == 'Labour']\n",
    "\n",
    "    prop_conservative_climate_speeches = len(\n",
    "        conservative_climate_speeches) / len(climate_hansard)\n",
    "    prop_labour_climate_speeches = len(\n",
    "        labour_climate_speeches) / len(climate_hansard)\n",
    "\n",
    "    diff_prop_climate_speeches = prop_labour_climate_speeches - \\\n",
    "        prop_conservative_climate_speeches\n",
    "    diff_prop_climate_speeches_list.append(diff_prop_climate_speeches)\n",
    "\n",
    "# Plot the difference in proportion of speeches about climate change against thresholds\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, diff_prop_climate_speeches_list,\n",
    "         marker='o', label='Difference in Proportion')\n",
    "plt.axhline(y=diff_prop_speeches, color='b', linestyle='--',\n",
    "            label='Reference in all Speeches')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Difference in Proportion')\n",
    "plt.title('Hansard: Difference in Proportion of Speeches about Climate Change')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
