{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Procedural Terms Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes the [online index of Erskine May]('https://erskinemay.parliament.uk/browse/indexterms?page=1') to create a list of procedural terms used in the UK Parliament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting parliamentary procedural terms from the online index of Erskine May\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting terms from 23.html...\n",
      "Extracting terms from 35.html...\n",
      "Extracting terms from 9.html...\n",
      "Extracting terms from 19.html...\n",
      "Extracting terms from 39.html...\n",
      "Extracting terms from 5.html...\n",
      "Extracting terms from 15.html...\n",
      "Extracting terms from 42.html...\n",
      "Extracting terms from 54.html...\n",
      "Extracting terms from 43.html...\n",
      "Extracting terms from 14.html...\n",
      "Extracting terms from 4.html...\n",
      "Extracting terms from 38.html...\n",
      "Extracting terms from 18.html...\n",
      "Extracting terms from 8.html...\n",
      "Extracting terms from 34.html...\n",
      "Extracting terms from 22.html...\n",
      "Extracting terms from 29.html...\n",
      "Extracting terms from 3.html...\n",
      "Extracting terms from 13.html...\n",
      "Extracting terms from 44.html...\n",
      "Extracting terms from 52.html...\n",
      "Extracting terms from 25.html...\n",
      "Extracting terms from 33.html...\n",
      "Extracting terms from 48.html...\n",
      "Extracting terms from 49.html...\n",
      "Extracting terms from 32.html...\n",
      "Extracting terms from 24.html...\n",
      "Extracting terms from 53.html...\n",
      "Extracting terms from 45.html...\n",
      "Extracting terms from 12.html...\n",
      "Extracting terms from 2.html...\n",
      "Extracting terms from 28.html...\n",
      "Extracting terms from 50.html...\n",
      "Extracting terms from 46.html...\n",
      "Extracting terms from 11.html...\n",
      "Extracting terms from 1.html...\n",
      "Extracting terms from 31.html...\n",
      "Extracting terms from 27.html...\n",
      "Extracting terms from 26.html...\n",
      "Extracting terms from 30.html...\n",
      "Extracting terms from 10.html...\n",
      "Extracting terms from 47.html...\n",
      "Extracting terms from 51.html...\n",
      "Extracting terms from 37.html...\n",
      "Extracting terms from 21.html...\n",
      "Extracting terms from 40.html...\n",
      "Extracting terms from 17.html...\n",
      "Extracting terms from 7.html...\n",
      "Extracting terms from 6.html...\n",
      "Extracting terms from 16.html...\n",
      "Extracting terms from 41.html...\n",
      "Extracting terms from 20.html...\n",
      "Extracting terms from 36.html...\n"
     ]
    }
   ],
   "source": [
    "def extract_terms(html_file_path, filename):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "\n",
    "    index_terms = soup.find_all('span', class_='text')\n",
    "    return [(term.get_text(strip=True), filename) for term in index_terms]\n",
    "\n",
    "\n",
    "def extract_terms_from_files(directory):\n",
    "    terms = []\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".html\"):\n",
    "            print(f'Extracting terms from {filename}...')\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            terms.extend(extract_terms(file_path, filename))\n",
    "    return terms\n",
    "\n",
    "\n",
    "directory = DATA_PATH + 'erskine-may-index/'\n",
    "index_terms = extract_terms_from_files(directory)\n",
    "index_terms_df = pd.DataFrame(index_terms, columns=['term', 'source_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning the procedural terms into a dictionary of stemmed unique unigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for the procedural terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    # Text should almost always be a string, but we check\n",
    "    # just in case\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and stemming the procedural terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms_df['cleaned_term'] = index_terms_df['term'].apply(clean_tokenize)\n",
    "exploded_index_terms_df = index_terms_df.explode('cleaned_term')\n",
    "unique_terms = exploded_index_terms_df['cleaned_term'].unique()\n",
    "unique_terms_df = pd.DataFrame(\n",
    "    unique_terms, columns=['term']).sort_values(by='term').reset_index(drop=True)\n",
    "\n",
    "unique_terms_df.to_csv(\n",
    "    DIST_PATH + 'hansard_procedural_terms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is manually cleaned at this point to create the final [`shortened_hansard_procedural_terms.csv` file](https://docs.google.com/spreadsheets/d/1twVZ_ypcBOLroMDxgbC0veFKvHq7BbT9HbW99zUnNU8/edit?usp=sharing).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
