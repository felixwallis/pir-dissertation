{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Procedural Terms Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes the [online index of Erskine May]('https://erskinemay.parliament.uk/browse/indexterms?page=1') to create a list of procedural terms used in the UK Parliament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting parliamentary procedural terms from the online index of Erskine May\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting terms from 23.html...\n",
      "Extracting terms from 35.html...\n",
      "Extracting terms from 9.html...\n",
      "Extracting terms from 19.html...\n",
      "Extracting terms from 39.html...\n",
      "Extracting terms from 5.html...\n",
      "Extracting terms from 15.html...\n",
      "Extracting terms from 42.html...\n",
      "Extracting terms from 54.html...\n",
      "Extracting terms from 43.html...\n",
      "Extracting terms from 14.html...\n",
      "Extracting terms from 4.html...\n",
      "Extracting terms from 38.html...\n",
      "Extracting terms from 18.html...\n",
      "Extracting terms from 8.html...\n",
      "Extracting terms from 34.html...\n",
      "Extracting terms from 22.html...\n",
      "Extracting terms from 29.html...\n",
      "Extracting terms from 3.html...\n",
      "Extracting terms from 13.html...\n",
      "Extracting terms from 44.html...\n",
      "Extracting terms from 52.html...\n",
      "Extracting terms from 25.html...\n",
      "Extracting terms from 33.html...\n",
      "Extracting terms from 48.html...\n",
      "Extracting terms from 49.html...\n",
      "Extracting terms from 32.html...\n",
      "Extracting terms from 24.html...\n",
      "Extracting terms from 53.html...\n",
      "Extracting terms from 45.html...\n",
      "Extracting terms from 12.html...\n",
      "Extracting terms from 2.html...\n",
      "Extracting terms from 28.html...\n",
      "Extracting terms from 50.html...\n",
      "Extracting terms from 46.html...\n",
      "Extracting terms from 11.html...\n",
      "Extracting terms from 1.html...\n",
      "Extracting terms from 31.html...\n",
      "Extracting terms from 27.html...\n",
      "Extracting terms from 26.html...\n",
      "Extracting terms from 30.html...\n",
      "Extracting terms from 10.html...\n",
      "Extracting terms from 47.html...\n",
      "Extracting terms from 51.html...\n",
      "Extracting terms from 37.html...\n",
      "Extracting terms from 21.html...\n",
      "Extracting terms from 40.html...\n",
      "Extracting terms from 17.html...\n",
      "Extracting terms from 7.html...\n",
      "Extracting terms from 6.html...\n",
      "Extracting terms from 16.html...\n",
      "Extracting terms from 41.html...\n",
      "Extracting terms from 20.html...\n",
      "Extracting terms from 36.html...\n"
     ]
    }
   ],
   "source": [
    "def extract_terms(html_file_path, filename):\n",
    "    \"\"\"\n",
    "    This function extracts the procedural terms from the HTML content of a page of \n",
    "    the Erskine May index and includes the source filename.\n",
    "    \"\"\"\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "\n",
    "    index_terms = soup.find_all('span', class_='text')\n",
    "    return [(term.get_text(strip=True), filename) for term in index_terms]\n",
    "\n",
    "\n",
    "def extract_terms_from_files(directory):\n",
    "    \"\"\"\n",
    "    This function iterates over HTML files in a given directory and extracts terms from each,\n",
    "    including the filename from which each term came.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".html\"):\n",
    "            print(f'Extracting terms from {filename}...')\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            terms.extend(extract_terms(file_path, filename))\n",
    "    return terms\n",
    "\n",
    "\n",
    "directory = DATA_PATH + 'erskine-may-index/'\n",
    "index_terms = extract_terms_from_files(directory)\n",
    "index_terms_df = pd.DataFrame(index_terms, columns=['term', 'source_file'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
