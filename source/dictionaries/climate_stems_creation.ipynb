{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Stems Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/5xzy77v17m1fbt0xq_sn3f140000gn/T/ipykernel_14462/3323969170.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixwallis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import fitz\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DIST_PATH = 'dist/'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting climate terms and their definitions from the IPCC Sixth Assessment Report Glossary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the text from the IPCC Sixth Assessment Report Glossary PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_path = DATA_PATH + 'IPCC Sixth Assessment Report Glossary.pdf'\n",
    "doc = fitz.open(glossary_path)\n",
    "\n",
    "# Remomving the first and final three pages of the glossary\n",
    "glossary_text = [page.get_text() for page in doc]\n",
    "glossary_text.pop(0)\n",
    "glossary_text = glossary_text[:-3]\n",
    "\n",
    "# Concatenating the glossary text\n",
    "glossary_text_string = ' '.join(glossary_text)\n",
    "\n",
    "# Removing superfluous text\n",
    "pattern = r\"(Approval Session|Glossary|IPCC SR1\\.5|Do Not Cite, Quote or Distribute|Total pages: \\d+|See [A-Za-z]+\\.|1-\\d+)\"\n",
    "cleaned_glossary_text = re.sub(pattern, '', glossary_text_string).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a rough dictionary of climate terms and their definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = re.split(r'\\s{5,}', cleaned_glossary_text)\n",
    "\n",
    "terms = []\n",
    "definitions = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    split_chunk = re.split(r'\\s{2,}', chunk)\n",
    "    term = split_chunk[0]\n",
    "    definition = ' '.join(split_chunk[1:])\n",
    "\n",
    "    terms.append(term)\n",
    "    definitions.append(definition)\n",
    "\n",
    "climate_dictionary = pd.DataFrame({'term': terms, 'definition': definitions})\n",
    "climate_dictionary = climate_dictionary.drop_duplicates(\n",
    "    subset='term', keep='first')\n",
    "\n",
    "climate_dictionary.to_csv(\n",
    "    DATA_PATH + 'rough_climate_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is manually cleaned at this point to create the [`cleaned_climate_dictionary.csv` file](https://docs.google.com/spreadsheets/d/1a1rvYR6gQWmUY9fYlmm2lxK9xqiRB0I6LqsUNNZ6eUw/edit#gid=1267656563).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning the climate terms into a dictionary of stemmed unique unigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for the climate terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    # Text should almost always be a string, but we check\n",
    "    # just in case\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and stemming the climate terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_climate_dictionary = pd.read_csv(\n",
    "    DATA_PATH + 'cleaned_climate_dictionary.csv')\n",
    "\n",
    "cleaned_climate_dictionary['cleaned_term'] = cleaned_climate_dictionary['term'].apply(\n",
    "    clean_tokenize)\n",
    "exploded_climate_terms_df = cleaned_climate_dictionary.explode('cleaned_term')\n",
    "unique_climate_terms = exploded_climate_terms_df['cleaned_term'].unique()\n",
    "unique_climate_terms_df = pd.DataFrame(unique_climate_terms, columns=[\n",
    "                                       'stem']).sort_values(by='stem').reset_index(drop=True)\n",
    "\n",
    "unique_climate_terms_df.to_csv(\n",
    "    DIST_PATH + 'climate_stems.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
